{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41a3fcf",
   "metadata": {},
   "source": [
    "How close can we get our mel2vec to fit to the psuedo-labels generated perch? Hopefully the answer is close..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34681c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema([('index', Int64),\n",
       "        ('file', String),\n",
       "        ('timestamp', Float64),\n",
       "        ('mfcc', List(Float32)),\n",
       "        ('part', Int64),\n",
       "        ('start_time', Int64)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import faiss\n",
    "\n",
    "\n",
    "# we'll join the two datasets on start time with a udf\n",
    "def get_start_time(timestamp, interval=5) -> int:\n",
    "    # up to but not including the value\n",
    "    for i in range(0, 100, interval):\n",
    "        if i <= timestamp < i + interval:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "shared_root = Path(\"~/shared/birdclef\").expanduser()\n",
    "scratch_root = Path(\"~/scratch/birdclef\").expanduser()\n",
    "\n",
    "# let's join this with the data that we have for the mfcc dataset\n",
    "mfcc = pl.scan_parquet(f\"{scratch_root}/2025/mfcc-soundscape/data\").with_columns(\n",
    "    pl.col(\"timestamp\")\n",
    "    .map_elements(get_start_time, return_dtype=pl.Int64)\n",
    "    .alias(\"start_time\")\n",
    ")\n",
    "display(mfcc.collect_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf9231c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6122, 13688, 1185, 9798, 4637, 10836, 6358, 9107, 10603, 12453]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer\n",
    "centroids = np.load(f\"{scratch_root}/2025/mel2vec/tokenizer/centroids.npy\")\n",
    "index = faiss.IndexFlatL2(centroids.shape[1])\n",
    "index.add(centroids)\n",
    "\n",
    "prefix = \"tokenizer=tokenizer/vector_size=256/window=80/ns_exponent=0.75/sample=0.0001/epochs=100\"\n",
    "word_vectors = KeyedVectors.load(\n",
    "    f\"{scratch_root}/2025/mel2vec/word2vec/{prefix}/word2vec.wordvectors\"\n",
    ")\n",
    "display(word_vectors.index_to_key[:10])\n",
    "\n",
    "\n",
    "def mfcc_to_wv(mfcc: list) -> list:\n",
    "    # convert mfcc to word vectors\n",
    "    X = np.array(mfcc).reshape(1, -1)\n",
    "    _, indices = index.search(X, 1)  # get the closest centroid\n",
    "    return word_vectors[indices[0][0]].tolist()\n",
    "\n",
    "\n",
    "def aggregate_mfcc(group: pl.DataFrame) -> pl.DataFrame:\n",
    "    X_mfcc = np.stack(group.get_column(\"mfcc\").to_numpy())\n",
    "    X_w2v = np.stack(group.get_column(\"word_vector\").to_numpy())\n",
    "    return pl.DataFrame(\n",
    "        {\n",
    "            \"file\": group.get_column(\"file\").to_numpy()[0],\n",
    "            \"start_time\": group.get_column(\"start_time\").to_numpy()[0],\n",
    "            \"mfcc_stats\": [X_mfcc.mean(axis=0).tolist() + X_mfcc.std(axis=0).tolist()],\n",
    "            \"word_vector\": [X_w2v.mean(axis=0).tolist()],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = (\n",
    "    mfcc.with_columns(\n",
    "        pl.col(\"mfcc\")\n",
    "        .map_elements(mfcc_to_wv, return_dtype=pl.List(pl.Float64))\n",
    "        .alias(\"word_vector\")\n",
    "    )\n",
    "    .group_by(\"file\", \"start_time\")\n",
    "    .map_groups(\n",
    "        aggregate_mfcc,\n",
    "        schema=pl.Schema(\n",
    "            {\n",
    "                \"file\": pl.Utf8,\n",
    "                \"start_time\": pl.Int64,\n",
    "                \"mfcc_stats\": pl.List(pl.Float64),\n",
    "                \"word_vector\": pl.List(pl.Float64),\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "    .sort(\"file\", \"start_time\")\n",
    ")\n",
    "\n",
    "# write this to parquet\n",
    "processed.sink_parquet(\n",
    "    f\"{scratch_root}/2025/mel2vec/mfcc-word-vector\",\n",
    "    compression=\"zstd\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc3949",
   "metadata": {},
   "source": [
    "Well, we put this in a script because it takes a while to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f5d3f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
