{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093bf2a3",
   "metadata": {},
   "source": [
    "Need to figure out how to speed up things like perch. Let's do some basic benchmarking to openvino can speed things up by an order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "428147aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-05T20:45:28.184902\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "print(datetime.datetime.now().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a0e8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioacoustics_model_zoo as bmz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/storage/home/hcoda1/8/amiyaguchi3/scratch/birdclef/test_soundscape/H02_20230420_074000.ogg'),\n",
       " PosixPath('/storage/home/hcoda1/8/amiyaguchi3/scratch/birdclef/test_soundscape/H02_20230420_112000.ogg')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "audio = sorted(Path(\"~/scratch/birdclef/test_soundscape\").expanduser().glob(\"*.ogg\"))[\n",
    "    :2\n",
    "]\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a019cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:599: UserWarning: \n",
      "                    This architecture is not listed in opensoundscape.ml.cnn_architectures.ARCH_DICT.\n",
      "                    It will not be available for loading after saving the model with .save() (unless using pickle=True). \n",
      "                    To make it re-loadable, define a function that generates the architecture from arguments: (n_classes, n_channels) \n",
      "                    then use opensoundscape.ml.cnn_architectures.register_architecture() to register the generating function.\n",
      "\n",
      "                    The function can also set the returned object's .constructor_name to the registered string key in ARCH_DICT\n",
      "                    to avoid this warning and ensure it is reloaded correctly by opensoundscape.ml.load_model().\n",
      "\n",
      "                    See opensoundscape.ml.cnn_architectures module for examples of constructor functions\n",
      "                    \n",
      "  warnings.warn(\n",
      "/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/opensoundscape/ml/cnn.py:623: UserWarning: Failed to detect expected # input channels of this architecture.Make sure your architecture expects the number of channels equal to `channels` argument 1). Pytorch architectures generally expect 3 channels by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perch(\n",
       "  (network): MLPClassifier(\n",
       "    (hidden_layers): Sequential()\n",
       "    (classifier): Linear(in_features=1280, out_features=10932, bias=True)\n",
       "  )\n",
       "  (loss_fn): BCEWithLogitsLoss_hot()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perch = bmz.list_models()[\"Perch\"]()\n",
    "perch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8289e3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501027668bad46ddbf8c1f667a5a271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749167273.520062 2333959 service.cc:146] XLA service 0x1ca947b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749167273.520123 2333959 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "2025-06-05 19:47:53.890956: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1749167273.904441 2333959 assert_op.cc:38] Ignoring Assert operator jax2tf_infer_fn_/assert_equal_1/Assert/AssertGuard/Assert\n",
      "I0000 00:00:1749167277.899508 2333959 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a6653731574fd9b3fb9e0f59d6a338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 s, sys: 98.3 ms, total: 34.2 s\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "# NOTE: there is a warmup period for the model...\n",
    "perch.predict(audio[0:1])\n",
    "%time _ = perch.predict(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "446aa316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00750089,  0.0125857 ,  0.00266587, ...,  0.00353971,\n",
       "          0.01147204,  0.00857955],\n",
       "        [ 0.00235424,  0.01088984, -0.0007503 , ...,  0.00444742,\n",
       "         -0.00075508,  0.00074249]], dtype=float32),\n",
       " array([], shape=(2, 0), dtype=bool))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scratch_root = Path(\"~/scratch/birdclef\").expanduser()\n",
    "\n",
    "for batch in perch.train_dataloader(audio, batch_size=2):\n",
    "    break\n",
    "batch[0].shape\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d6d8274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method train_dataloader in module opensoundscape.ml.cnn:\n",
      "\n",
      "train_dataloader(samples, bypass_augmentations=False, collate_fn=<function collate_audio_samples at 0x7ffe655f4430>, **kwargs) method of bioacoustics_model_zoo.perch.Perch instance\n",
      "    generate dataloader for training\n",
      "    \n",
      "    train_loader samples batches of images + labels from training set\n",
      "    \n",
      "    Args: see self.train_dataloader_cls docstring for arguments\n",
      "        **kwargs: any arguments to pass to the DataLoader __init__\n",
      "        Note: some arguments are fixed and should not be passed in kwargs:\n",
      "        - shuffle=True: shuffle samples for training\n",
      "        - bypass_augmentations=False: apply augmentations to training samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(perch.train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ab99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(_SignatureMap({'serving_default': <ConcreteFunction (*, inputs: TensorSpec(shape=(None, 160000), dtype=tf.float32, name='inputs')) -> Dict[['order', TensorSpec(shape=(None, 41), dtype=tf.float32, name='order')], ['embedding', TensorSpec(shape=(None, 1280), dtype=tf.float32, name='embedding')], ['family', TensorSpec(shape=(None, 249), dtype=tf.float32, name='family')], ['frontend', TensorSpec(shape=(None, 500, 160), dtype=tf.float32, name='frontend')], ['genus', TensorSpec(shape=(None, 2333), dtype=tf.float32, name='genus')], ['label', TensorSpec(shape=(None, 10932), dtype=tf.float32, name='label')]] at 0x7FFFE411B310>}))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"embedding\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"embedding\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ waveform_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160000</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ waveform_input (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160000\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mLambda\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "display(perch.tf_model.signatures.keys())\n",
    "input_layer = tf.keras.layers.Input(\n",
    "    shape=(160000,), dtype=tf.float32, name=\"waveform_input\"\n",
    ")\n",
    "embedding_fn = perch.tf_model.signatures[\"serving_default\"]\n",
    "embedding_output = tf.keras.layers.Lambda(\n",
    "    lambda x: embedding_fn(inputs=x)[\"embedding\"],\n",
    "    name=\"embedding\",\n",
    "    output_shape=embedding_fn.structured_outputs[\"embedding\"].shape,\n",
    ")(input_layer)\n",
    "core_embedding_model = tf.keras.Model(\n",
    "    inputs=input_layer,\n",
    "    outputs=embedding_output,\n",
    "    name=\"embedding\",\n",
    ")\n",
    "core_embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b3f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /storage/home/hcoda1/8/amiyaguchi3/scratch/birdclef/compiled/perch_embedding_extractor/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /storage/home/hcoda1/8/amiyaguchi3/scratch/birdclef/compiled/perch_embedding_extractor/assets\n"
     ]
    }
   ],
   "source": [
    "class PerchEmbeddingExtractor(tf.Module):\n",
    "    def __init__(self, perch_model_object):\n",
    "        super().__init__()\n",
    "        self.original_model = perch_model_object\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[tf.TensorSpec(shape=[None, 160000], dtype=tf.float32)]\n",
    "    )\n",
    "    def __call__(self, waveform):\n",
    "        \"\"\"\n",
    "        This function defines the new, pruned computation graph.\n",
    "        \"\"\"\n",
    "        output_dict = self.original_model.signatures[\"serving_default\"](inputs=waveform)\n",
    "        return {\"embedding\": output_dict[\"embedding\"]}\n",
    "\n",
    "\n",
    "# 3. Instantiate our new, clean module\n",
    "pruned_model = PerchEmbeddingExtractor(perch.tf_model)\n",
    "tf.saved_model.save(pruned_model, scratch_root / \"compiled/perch_embedding_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e82eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkpo5umb9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkpo5umb9/assets\n",
      "W0000 00:00:1749170228.875034 2333959 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1749170228.875061 2333959 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2025-06-05 20:37:08.875300: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpkpo5umb9\n",
      "2025-06-05 20:37:08.898063: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-06-05 20:37:08.898093: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpkpo5umb9\n",
      "2025-06-05 20:37:09.181191: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-06-05 20:37:10.278259: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpkpo5umb9\n",
      "2025-06-05 20:37:10.729380: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 1854083 microseconds.\n",
      "2025-06-05 20:37:20.161524: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3463] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexEnsureShape, FlexStridedSlice, FlexTranspose\n",
      "Details:\n",
      "\ttf.EnsureShape(tensor<?x?x?xcomplex<f32>>) -> (tensor<?x513x500xcomplex<f32>>) : {device = \"\", shape = #tf_type.shape<?x513x500>}\n",
      "\ttf.EnsureShape(tensor<?x?x?xf32>) -> (tensor<?x1x160xf32>) : {device = \"\", shape = #tf_type.shape<?x1x160>}\n",
      "\ttf.StridedSlice(tensor<?x513x501xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<?x?x?xcomplex<f32>>) : {begin_mask = 0 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}\n",
      "\ttf.Transpose(tensor<?x501x513xcomplex<f32>>, tensor<3xi32>) -> (tensor<?x513x501xcomplex<f32>>) : {device = \"\"}\n",
      "\ttf.Transpose(tensor<?x513x500xcomplex<f32>>, tensor<3xi32>) -> (tensor<?x500x513xcomplex<f32>>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(perch.tf_model)\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(\n",
    "#     (scratch_root / \"compiled/perch_embedding_extractor\").as_posix()\n",
    "# )\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TFLite native ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,  # Enable TF ops as a fallback.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "with (scratch_root / \"compiled/perch.tflite\").open(\"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06ae49d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 160000)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "60413f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 20:47:03.848530: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_inter_op_parallelism which is not in the op definition: Op<name=Transpose; signature=x:T, perm:Tperm -> y:T; attr=T:type; attr=Tperm:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node Transpose}}\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.38 s, sys: 41 ms, total: 3.42 s\n",
      "Wall time: 3.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# does this work any better than the original model?\n",
    "interpreter = tf.lite.Interpreter(\n",
    "    model_path=(scratch_root / \"compiled/perch.tflite\").as_posix()\n",
    ")\n",
    "interpreter.allocate_tensors()  # This is a mandatory step\n",
    "\n",
    "\n",
    "def run_predict(interpreter, dataloader):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    res = []\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        interpreter.set_tensor(input_details[0][\"index\"], batch[0])\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[1][\"index\"])\n",
    "        res.append(output_data)\n",
    "    return np.stack(res)\n",
    "\n",
    "\n",
    "%time _ = run_predict(interpreter, perch.train_dataloader(audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53e785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-05 20:31:41.691339: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-05 20:31:41.709347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-05 20:31:41.731101: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-05 20:31:41.737690: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-05 20:31:41.754000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-05 20:31:43.698384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.10.10-3udzu6x4ehl4hglrzjzujjadkhcf7vvz/lib/python3.10/runpy.py:126: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2025-06-05 20:31:47,813 - INFO - Using tensorflow=2.17.0, onnx=1.17.0, tf2onnx=1.16.1/15c810\n",
      "2025-06-05 20:31:47,813 - INFO - Using opset <onnx, 18>\n",
      "INFO: Created TensorFlow Lite delegate for select TF ops.\n",
      "INFO: TfLiteFlexDelegate delegate: 5 nodes delegated out of 537 nodes with 3 partitions.\n",
      "\n",
      "2025-06-05 20:31:47.830221: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_inter_op_parallelism which is not in the op definition: Op<name=Transpose; signature=x:T, perm:Tperm -> y:T; attr=T:type; attr=Tperm:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node Transpose}}\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2025-06-05 20:31:48,207 - ERROR - Failed to convert node 'jax2tf_infer_fn_/TaxonomyModel/frontend/pjit_matmul_/einsum/Einsum' (fct=<bound method TflFullyConnectedOp.to_tf of <class 'tf2onnx.tflite_handlers.tfl_math.TflFullyConnectedOp'>>)\n",
      "'OP=TFL_FULLY_CONNECTED\\nName=jax2tf_infer_fn_/TaxonomyModel/frontend/pjit_matmul_/einsum/Einsum\\nInputs:\\n\\tjax2tf_infer_fn_/TaxonomyModel/frontend/pjit__power_/Pow;=Mul, [-1, 500, 513], 1\\n\\ttfl.pseudo_qconst80_dequant=TFL_DEQUANTIZE, [160, 513], 1\\nOutpus:\\n\\tjax2tf_infer_fn_/TaxonomyModel/frontend/pjit_matmul_/einsum/Einsum=[-1, 500, 160], 1'\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tfonnx.py\", line 292, in tensorflow_onnx_mapping\n",
      "    func(g, node, **kwargs, initialized_tables=initialized_tables, dequantize=dequantize)\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tflite_handlers/tfl_math.py\", line 204, in to_tf\n",
      "    utils.make_sure(node.attr['keep_num_dims'].i == 0,\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/utils.py\", line 303, in make_sure\n",
      "    raise ValueError(\"make_sure failure: \" + error_msg % args)\n",
      "ValueError: make_sure failure: Only keep_num_dims=False supported for fully connected op\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.10.10-3udzu6x4ehl4hglrzjzujjadkhcf7vvz/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/python-3.10.10-3udzu6x4ehl4hglrzjzujjadkhcf7vvz/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/convert.py\", line 714, in <module>\n",
      "    main()\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/convert.py\", line 273, in main\n",
      "    model_proto, _ = _convert_common(\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/convert.py\", line 168, in _convert_common\n",
      "    g = process_tf_graph(tf_graph, const_node_values=const_node_values,\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tfonnx.py\", line 464, in process_tf_graph\n",
      "    g = process_graphs(main_g, subgraphs, custom_op_handlers, inputs_as_nchw, outputs_as_nchw, continue_on_error,\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tfonnx.py\", line 516, in process_graphs\n",
      "    g = process_parsed_graph(main_g, custom_op_handlers, inputs_as_nchw, outputs_as_nchw, continue_on_error,\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tfonnx.py\", line 537, in process_parsed_graph\n",
      "    raise exceptions[0]\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tfonnx.py\", line 292, in tensorflow_onnx_mapping\n",
      "    func(g, node, **kwargs, initialized_tables=initialized_tables, dequantize=dequantize)\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/tflite_handlers/tfl_math.py\", line 204, in to_tf\n",
      "    utils.make_sure(node.attr['keep_num_dims'].i == 0,\n",
      "  File \"/storage/home/hcoda1/8/amiyaguchi3/clef/birdclef-2025/.venv/lib/python3.10/site-packages/tf2onnx/utils.py\", line 303, in make_sure\n",
      "    raise ValueError(\"make_sure failure: \" + error_msg % args)\n",
      "ValueError: make_sure failure: Only keep_num_dims=False supported for fully connected op\n"
     ]
    }
   ],
   "source": [
    "! python -m tf2onnx.convert \\\n",
    "    --tflite {scratch_root / \"compiled/perch.tflite\"} \\\n",
    "    --output {scratch_root / \"compiled/perch.onnx\"} \\\n",
    "    --opset 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69e7d733",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'translate_map.count(decoder->get_op_type())' failed at src/frontends/tensorflow_lite/src/frontend.cpp:242:\nFrontEnd API failed with OpConversionFailure:\nNo translator found for FlexTranspose node.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpConversionFailure\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenvino\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mov\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ov.convert_model(perch.tf_model, example_input=batch[0])\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ove_model \u001b[38;5;241m=\u001b[39m \u001b[43mov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscratch_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompiled/perch.tflite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# No translator found for FlexTranspose node.\u001b[39;00m\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/convert.py:105\u001b[0m, in \u001b[0;36mconvert_model\u001b[0;34m(input_model, input, output, example_input, extension, verbose, share_weights)\u001b[0m\n\u001b[1;32m    103\u001b[0m logger_state \u001b[38;5;241m=\u001b[39m get_logger_state()\n\u001b[1;32m    104\u001b[0m cli_parser \u001b[38;5;241m=\u001b[39m get_all_cli_parser()\n\u001b[0;32m--> 105\u001b[0m ov_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcli_parser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m restore_logger_state(logger_state)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/convert_impl.py:571\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[1;32m    569\u001b[0m send_conversion_result(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfail\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m python_api_used:\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, argv\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/convert_impl.py:511\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(cli_parser, args, python_api_used)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_pytorch_decoder_for_model_on_disk(argv, args):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# try to load a model from disk as TorchScript or ExportedProgram\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# TorchScriptPythonDecoder or TorchFXPythonDecoder object will be assigned to argv.input_model\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# saved TorchScript and ExportedModel model can be passed to both ovc tool and Python convert_model\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     pytorch_model_on_disk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversion_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pytorch_model_on_disk:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# release memory allocated for temporal object\u001b[39;00m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m argv\u001b[38;5;241m.\u001b[39minput_model\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/convert_impl.py:249\u001b[0m, in \u001b[0;36mdriver\u001b[0;34m(argv, non_default_params)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdriver\u001b[39m(argv: argparse\u001b[38;5;241m.\u001b[39mNamespace, non_default_params: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Log dictionary with non-default cli parameters where complex classes are excluded.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;28mstr\u001b[39m(non_default_params))\n\u001b[0;32m--> 249\u001b[0m     ov_model \u001b[38;5;241m=\u001b[39m moc_emit_ir(\u001b[43mprepare_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m, argv)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/convert_impl.py:195\u001b[0m, in \u001b[0;36mprepare_ir\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m extension \u001b[38;5;129;01min\u001b[39;00m filtered_extensions(argv\u001b[38;5;241m.\u001b[39mextension):\n\u001b[1;32m    194\u001b[0m             moc_front_end\u001b[38;5;241m.\u001b[39madd_extension(extension)\n\u001b[0;32m--> 195\u001b[0m     ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mmoc_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmoc_front_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m argv\u001b[38;5;241m.\u001b[39minput_model:\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/tools/ovc/moc_frontend/pipeline.py:293\u001b[0m, in \u001b[0;36mmoc_pipeline\u001b[0;34m(argv, moc_front_end)\u001b[0m\n\u001b[1;32m    289\u001b[0m             input_model\u001b[38;5;241m.\u001b[39mset_partial_shape(place, ov_shape)\n\u001b[1;32m    291\u001b[0m         input_model\u001b[38;5;241m.\u001b[39mset_tensor_value(place, value)\n\u001b[0;32m--> 293\u001b[0m ov_model \u001b[38;5;241m=\u001b[39m \u001b[43mmoc_front_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ov_model\n",
      "File \u001b[0;32m~/clef/birdclef-2025/.venv/lib/python3.10/site-packages/openvino/frontend/frontend.py:18\u001b[0m, in \u001b[0;36mFrontEnd.convert\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: Union[Model, InputModel]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[0;32m---> 18\u001b[0m     converted_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, InputModel):\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Model(converted_model)\n",
      "\u001b[0;31mOpConversionFailure\u001b[0m: Check 'translate_map.count(decoder->get_op_type())' failed at src/frontends/tensorflow_lite/src/frontend.cpp:242:\nFrontEnd API failed with OpConversionFailure:\nNo translator found for FlexTranspose node.\n"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# ov.convert_model(perch.tf_model, example_input=batch[0])\n",
    "ove_model = ov.convert_model((scratch_root / \"compiled/perch.tflite\").as_posix())\n",
    "# No translator found for FlexTranspose node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
